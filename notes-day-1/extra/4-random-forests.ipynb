{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance from the leaves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have already seen the problem of overfitting in decision trees.  The problem of overfitting is a problem of variance.  There is randomness in our training data, and our decision tree perfectly matches this randomness, but then does not do so well when this randomness is not repeated.  The way that a decision tree can perfectly match the data in the training set, is because each leaf node predicting a small number of samples (maybe just one).  But because each prediction is made on a sample size of just one, these predictions are not necessarily predictive of future data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we saw a solution for correcting for this overfitting, which is prevent our decision tree from splitting before the leafs are left with just one sample.  This solution, does a fine job of overfitting in the leaf nodes, but unfortunately, we have not solved the problem of variance all together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beyond the leaf nodes, there are other reasons why decision trees are highly susceptible to variance.  With decision trees, any split made at one layer can affect the splits made at each subsequent layers.  So if we made a separate split, we could have arrived at a totally different tree.  Let's see how variance can play a roll."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's work with our Airbnb dataset again.  We have already cleaned our data, so we just need to load it into our notebook, and then can feed it into a decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_feather('cleaned_df.feather')\n",
    "X = df.drop(columns=['price'])\n",
    "y = df.price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training our trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The first tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our first decision tree, we simply split our data into training and test sets fit our tree. Notice that we are limiting our tree to a max depth of 3.  We are doing this to keep the tree relatively simple, so that we can plot the tree and then compare it against another tree later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=4, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(X, y, test_size=0.40, random_state=42)\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "dtr_1 = DecisionTreeRegressor(max_depth=4)\n",
    "dtr_1.fit(X_train_1, y_train_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The second tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the second tree the same way.  The only difference is that to ensure we are training on a different subset of the data, we change the value of our random state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=4, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X, y, test_size=0.40, random_state=21)\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "dtr_2 = DecisionTreeRegressor(max_depth=4)\n",
    "dtr_2.fit(X_train_2, y_train_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing our trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now let's compare our two trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first tree's middle two layers look like the following."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./dtr_1-a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is the second tree's middle two layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./dtr_2-a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So while there is some overlap between the attributes selected, half of the attributes in the second tree are new.  The differences in the trees leads to different predictions for the same datapoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Here, to correct for overfitting from either tree, we find the datapoints that were in the test sets of both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_both_test_sets = np.intersect1d(y_test_1.index, y_test_2.index)\n",
    "ten_in_both_tests = np.random.choice(in_both_test_sets, 10)\n",
    "ten_in_both_tests = np.array([  221,  7922, 17116, 11118,  2497,  4199,  8692,  5348, 14548,\n",
    "        1455])\n",
    "ten_X = X.iloc[ten_in_both_tests, :]\n",
    "ten_y = y[ten_in_both_tests]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we make predictions on the datapoints with the two trees and see how we do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtr_1_predictions = dtr_1.predict(ten_X)\n",
    "dtr_2_predictions = dtr_2.predict(ten_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dtr_1_predictions</th>\n",
       "      <th>dtr_2_predictions</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51.46048</td>\n",
       "      <td>69.814031</td>\n",
       "      <td>63.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51.46048</td>\n",
       "      <td>40.752675</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51.46048</td>\n",
       "      <td>69.814031</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>51.46048</td>\n",
       "      <td>69.814031</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51.46048</td>\n",
       "      <td>40.752675</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>106.64289</td>\n",
       "      <td>126.464427</td>\n",
       "      <td>95.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>51.46048</td>\n",
       "      <td>40.752675</td>\n",
       "      <td>43.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>51.46048</td>\n",
       "      <td>40.752675</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>51.46048</td>\n",
       "      <td>69.814031</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>51.46048</td>\n",
       "      <td>69.814031</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dtr_1_predictions  dtr_2_predictions      y\n",
       "0           51.46048          69.814031   63.0\n",
       "1           51.46048          40.752675   20.0\n",
       "2           51.46048          69.814031   45.0\n",
       "3           51.46048          69.814031   40.0\n",
       "4           51.46048          40.752675   22.0\n",
       "5          106.64289         126.464427   95.0\n",
       "6           51.46048          40.752675   43.0\n",
       "7           51.46048          40.752675   25.0\n",
       "8           51.46048          69.814031   42.0\n",
       "9           51.46048          69.814031  100.0"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "predictions_grid = np.hstack((dtr_1_predictions.reshape(-1, 1),\n",
    "                              dtr_2_predictions.reshape(-1, 1),\n",
    "                              ten_y.to_numpy().reshape(-1, 1))\n",
    "                             )\n",
    "\n",
    "df_predictions = pd.DataFrame(predictions_grid, columns = ['dtr_1_predictions', 'dtr_2_predictions', 'y'])\n",
    "df_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see from the dataframe above that from our two trees each of are making fairly different predictions, even though they both trained on most of the data, and we are using relatively simple trees.  So this our problem of variance: each tree is picking up on the random variations in the data, which leads to variations in our predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with our error of variance, we expect each model to deviate from the actual model, but if we were to take many of these mdoels we would expect the error of variance to be reduced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is the principal idea behind a random forest.  We begin with a machine learning algorithm of trees that suffers from variance, and we reduce the error due to variance by creating many trees and then taking the average of our various models predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we introduced the idea of random forest."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
