{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Another source of variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's work with our Airbnb dataset again.  We have already cleaned our data, so we just need to load it into our notebook, and then can feed it into a decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('types.json', 'r') as f:\n",
    "    data_types = json.load(f)\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_feather('cleaned_df.feather')\n",
    "X = df.drop(columns=['price'])\n",
    "y = df.price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training our trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=4, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(X, y, test_size=0.40, random_state=42)\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "dtr_1 = DecisionTreeRegressor(max_depth=4)\n",
    "dtr_1.fit(X_train_1, y_train_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The second tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train a second tree the same way.  To ensure our tree is different than the first, we'll  train on a different subset of the data by changing the value of our random state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=4, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X, y, test_size=0.40, random_state=21)\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "dtr_2 = DecisionTreeRegressor(max_depth=4)\n",
    "dtr_2.fit(X_train_2, y_train_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing our trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now let's compare our two trees.  As we'll see below, there is even a degree of variance in the early layers of the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_tree import plot_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing the different predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the different predictions of our two trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Below we select ten datapoints that are not in training sets of either tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "ten_in_both_tests = np.array([221,  7922, 17116, 11118,  2497,  4199,  8692,  5348, 14548, 1455])\n",
    "ten_X = X.iloc[ten_in_both_tests, :]\n",
    "ten_y = y[ten_in_both_tests]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now let's find the predictions of the two trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtr_1_predictions = dtr_1.predict(ten_X)\n",
    "dtr_2_predictions = dtr_2.predict(ten_X)\n",
    "df_predictions = pd.DataFrame({'dtr_1_predictions': dtr_1_predictions, \n",
    "                               'dtr_2_predictions': dtr_2_predictions, 'ten_y': ten_y.to_numpy()})\n",
    "# df_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recognize the Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Even more variance \n",
    "\n",
    "2. Why variant \n",
    "    * Few assumptions \n",
    "    * Greedy approach\n",
    "    \n",
    "3. Correcting for Variance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
